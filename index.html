
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>Inner Monologue</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- <base href="/"> -->

        <!--FACEBOOK-->
    <meta property="og:image:type" content="image/png">
    <meta property="og:image:width" content="682">
    <meta property="og:image:height" content="682">
    <meta property="og:type" content="website" />
    <meta property="og:url" content="https://inner-monologue.github.io/"/>
    <meta property="og:title" content="Inner Monologue" />
    <meta property="og:description" content="Project page for Inner Monologue: Embodied Reasoning through Planning with Language Models" />

        <!--TWITTER-->
    <meta name="twitter:card" content="summary_large_image" />
    <meta name="twitter:title" content="Inner Monologue" />
    <meta name="twitter:description" content="Project page for Project page for Inner Monologue: Embodied Reasoning through Planning with Language Models" />


<!--     <link rel="apple-touch-icon" href="apple-touch-icon.png"> -->
  <!-- <link rel="icon" type="image/png" href="img/seal_icon.png"> -->
    <!-- Place favicon.ico in the root directory -->

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>
    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h2 class="col-md-12 text-center">
                <b><font size="+4">Inner Monologue:</font></b> </br> Embodied Reasoning through Planning with Language Models</br> 
                <!--<small>
                    CoRL 2021
                </small>-->
            </h2>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
               CoRL 2022 Submission 370
            </div>
        </div>

    	<!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <div class="text-center">
                    <video id="v0" width="100%" playsinline muted loop controls>
                       <source src="img/demo1.mp4" type="video/mp4">
                   </video>
                </div>
            </div>
        </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
               
                <h3>
                    Abstract
                </h3>
                <p class="text-justify">
Recent works have shown how the reasoning capabilities of Large Language Models (LLMs) can be applied to domains beyond natural language processing, such as planning and interaction for robots. These embodied problems require an agent to under4 stand many semantic aspects of the world: the repertoire of skills available, how these skills influence the world, and how changes to the world map back to the language. LLMs plan6 ning in embodied environments need to consider not just what skills to do, but also how and when to do them - answers that change over time in response to the agent’s own choices. 

In this work, we investigate to what extent LLMs used in such embodied contexts can reason over sources of feedback provided through natural language, without any additional training. We propose that by leveraging environment feedback, LLMs are able to form an inner monologue that allows them to more richly process and plan in robotic control scenarios. We investigate a variety of sources of feedback, such as success detection, scene description, and human interaction. We find that closed-loop language feedback significantly improves high level instruction completion on three domains, including simulated and real table top rearrangement tasks and long-horizon mobile manipulation tasks in a kitchen environment.
                </p>
            
            </div>
        </div>


	<!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Supplementary Video
                </h3>
                <div class="text-center">
                    <video id="v0" width="100%" playsinline loop controls>
                       <source src="img/Inner_Monologue_supp_video_corl_compressed.mp4" type="video/mp4">
                   </video>
                </div>
            </div>
        </div> -->


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
            	<br>
                <h3>
                    Approach
                </h3>
                <p class="text-justify">
               We formulate an “inner monologue” by continually substituting information from the various sources of
 feedback into the LLM planning language prompts as the robot interacts with the environment. While LLMs
 have demonstrated exceptional planning capabilities for embodied control tasks, prior works have found
 it crucial to ground LLM predictions with external components such as affordance functions in order
 to produce useful plans that are executable by robots. However, LLMs used in this context have thus far
 remained one-directional – providing a list of skills, without making corrections or leveraging opportunities
 to re-plan accordingly. In contrast, Inner Monologue studies settings where grounded environment feedback
 is provided directly to the LLM in a closed-loop fashion. This promotes improved LLM reasoning in complex
 long-horizon settings, even before any external affordance-based grounding methods are applied.
   

                <p class="text-justify">
             <p style="text-align:center;">
                <image src="img/teaser.png"  class="img-responsive" height="600px">
            </p>
          
            
        
        </p>

            </div>
        </div>


        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Results
                </h3>
		<p class="text-justify">
	In order to study how different sources of environment feedback can support a rich inner monologue that en
ables complex robotic control, we analyze diverse long-horizon manipulation and navigation tasks in simulation
and in the real world. As Inner Monologue is not dependent on a specific LLM or a type of grounding feedback,
we study different Inner Monologue implementations in three environments with different LLM planning
methods and different sources of feedback from the environment. Below, we show results for a tabletop manip
ulation environment in sim and real, as well as a mobile manipulation environment
in real.
		</p>

        <p class="text-justify">
            Videos TBA
        </p>


		
		<p class="text-justify">
             <p style="text-align:center;">
                <image src="img/emergent1.png"  class="img-responsive" height="600px">
            </p>
             <p style="text-align:center;">
                <image src="img/emergent2.png"  class="img-responsive" height="600px">
            </p>
     
		</p>
		    

	    </div>
        </div>
            
     
     
    </div>
</body>
</html>
